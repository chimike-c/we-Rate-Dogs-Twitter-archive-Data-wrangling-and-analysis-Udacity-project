{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangle_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we wrangle WeRateDogs Twitter data to create interesting and trustworthy analyses and visualizations.\n",
    "\n",
    "The Twitter archive is great, but it only contains very basic tweet information. Additional gathering, then assessing and cleaning was then carried out in order to carry out analysis and visualisations. Various tentative conclusions/inferences/observations will be derived from said analysis. Some of the analysis carried out in this project is aimed at observing the accuracy and extent of the neural network used to predict dog breeds from the tweets. Also the level of user interaction with WeRateDogs tweets. More information like users favorite dog breed were analyzed. \n",
    "\n",
    "\n",
    "The data wrangling carried out in this project included various oprations; \n",
    "\n",
    "1. Data Gathering\n",
    "2. Assessing Data (noting down, quality and tidiness issues)\n",
    "3. Cleaning Data \n",
    "4. Storing Data \n",
    "\n",
    "\n",
    "\n",
    "#### Data Gathering\n",
    "\n",
    "For the project, data gathering was carried out in various manners i.e. data used was gathered from various sources using different methods. \n",
    "\n",
    "The first set of data was gathered by programatically downloading and reading a csv file which was a  twitter archive dataset containing data on a set of tweets by WeRateDogs page. This dataset was saved as 'twitter-archive-enhanced.csv'. \n",
    "\n",
    "The second set of data was gathered by using the requets library to programmatically download and read a tsv file gotten from a given url containing data on the tweet images of a set of WeRateDogs tweets. This dataset was saved as 'tweet_image_predictions.tsv'.\n",
    "\n",
    "The third set of data was gathered by using the tweepy library to query data from the twitter API. This dataset contained favorite count, retweet counts and other statistics on the same set of tweets as the first set of data. The dataset was saved as a JSON file 'tweet_json.txt'. \n",
    "\n",
    "\n",
    "#### Asessing Data\n",
    "\n",
    "For this project, the data was assesed both visually and programmatically, and detected data quality issues and data tidiness issues were noted down to be worked on before analysis. \n",
    "\n",
    "The following quality and tidiness issues were detected; \n",
    "\n",
    "Quality issues - \n",
    "\n",
    "1. drop non-original tweets rows i.e. tweets that are retweets and/or are tweets in reply to dog-rating tweets\n",
    "\n",
    "2. drop/correct rows with incorrect dog ratings numerator (index numbers- 2335,45, )\n",
    "\n",
    "3. rows with incorrect dog rating denominators \n",
    "\n",
    "4. correct timestamp data type (from object to timedate...)\n",
    "\n",
    "5. drop rows without tweet ids... (NaN as value ...)\n",
    "\n",
    "6. convert url(html file) to a df table\n",
    "\n",
    "7. note rows in df_2 where p1_dog, p2_dog or p3_dog  is false i.e when the prediction is not an actual dog breed\n",
    "\n",
    "9. image number column????\n",
    "\n",
    "10. drop 'in_reply_to_status_id ' and 'in_reply_to_user_id' columns (they are not useful for our analysis)\n",
    "\n",
    "11. drop 'retweeted_status_id' and 'retweeted_status_user_id' and 'retweeted_status_timestamp' column \n",
    "\n",
    "12. drop 'source' column.... (not needed for our analysis)\n",
    "\n",
    "13. convert tweet id column values in twitterapi data table from index form to plain values\n",
    "\n",
    "14. non essential rows in twitterapi data df, leaving only tweet id, favorite count and retweet count columns\n",
    "\n",
    "15. new tweet_id column should be created in the df_twitterapi_new table, to form and replace the former column in the table...\n",
    "\n",
    "\n",
    "Tidiness issues - \n",
    "\n",
    "1. dog type should be a column instaed of 4 different columns (doggo, floofer, puppo, pupper as variables...)\n",
    "\n",
    "2. predicted dog breed (predicted_breed) should be a column instead of having columns p1, p1_dog, p1_conf, p2, p2_dog, p2_conf, p3, p3_dog and p3_conf. The predicted dog breed column is formed by indicating the prediction with the highest confidence (p1, p2, p3).\n",
    "\n",
    "3. merge tables to form master dataset...\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Cleaning Data\n",
    "\n",
    "For this project, the detected quality and tidiness issues were programatically cleaned using various code blocks as seen in the warngle_act notebook. \n",
    "\n",
    "\n",
    "#### Storing Data \n",
    "\n",
    "In this section of the project, the cleaned datasets were saved and then merged to form a master dataset. The resulting clean data sets for each of the initial three datasets, df1_cleaned, df2_cleaned and dftwitterapi_cleaned were then merged programmatically to form 'twitter_archive_master'. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
